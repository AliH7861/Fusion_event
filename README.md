# Documentation of Submission of Julian and Ali

```bash
project_root/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input/         # Per-scene input metadata (JSON)
â”‚   â”œâ”€â”€ output/        # Per-scene ground-truth labels (JSON)
â”‚   â””â”€â”€ predictions/   # Model predictions (JSON)
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ dummy_model.py         # DummyModel class for testing
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ box_utils.py           # 3D box utilities (create_3d_box, compute_3d_iou, etc.)
â”‚   â””â”€â”€ json_utils.py          # JSON load/save utilities
â”‚
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ car_top_view.png       # Image asset for dashboard (used by frontend.py)
â”‚
â”œâ”€â”€ detect_objects.py          # Runs YOLO model for object detection
â”œâ”€â”€ frontend.py                # PyQt6 dashboard GUI
â”œâ”€â”€ test.py                    # Main pipeline: sensor fusion, prediction, evaluation, visualization
â”œâ”€â”€ visualize_scene.py         # Standalone visualization and IoU calculation script
â”‚
â”œâ”€â”€ README.md                 

```


# Fusion_event


## ğŸ” Problem

Autonomous vehicles rely on sensors like LiDAR and cameras to perceive their environment. Each sensor has strengths and limitations:

ğŸ”¹ LiDAR provides accurate 3D data but lacks color information.

ğŸ”¹ Cameras capture rich visuals but are sensitive to lighting conditions.


Individually, these sensors can be noisy or miss key details due to occlusions from other road agents. However, by fusing data from multiple sensors and vehicles, we can create a more reliable, comprehensive view of the scene, improving safety and awareness.

## ğŸš¦ Scenario

Two self-driving cars are approaching an intersection, each equipped with:


âœ… 3D LiDAR

âœ… Camera

The environment includes vehicles, pedestrians, and cyclists, some of whom may block each vehicleâ€™s view. By communicating and sharing sensor data, the vehicles can collaborate to overcome occlusions and enhance situational understanding.

![scene](/images/scene.png)

The camera Intrinsic Matrix is
``` 
  2058.72664   ,  0           , 960
  0            , 2058.72664   , 540
  0            , 0            , 1
```

Each lidar is mounted above the center of its vehicle by 1.7 meters. The camera, as well, is mounted 1.5 meters above the center. Both are directed towards the front of the car, parallel to the road level.

## Dataset Description

The [/data/input](/data/input) folder contains a JSON metadata file for each scene. Each JSON file contains the following information

| Field | Description | Unit |
| --- | --- | --- |
| CarA_Camera | Path to the image captured by the first vehicle's camera |  |
| CarA_Lidar | Path to the point cloud captured by the first vehicle's Lidar|  |
| CarA_Location | The location of the center of the first vehicle (x, y) | meters |
| CarA_Rotation | The rotation of the first vehicle | degree |
| CarA_Dimension | The dimensions of the first vehicle (Length, Width, Height) | m |
| CarB_Camera | Path to the image captured by the second vehicle's camera |  |
| CarB_Lidar | Path to the point cloud captured by the second vehicle's Lidar|  |
| CarB_Location | The location of the center of the second vehicle (x, y) | meters |
| CarB_Rotation | The rotation of the second vehicle | degree |
| CarB_Dimension | The dimensions of the second vehicle (Length, Width, Height) | meters |

The [/data/output](/data/output) folder contains JSON files of all other road agents in the scene. Each JSON file contains an array of 

| Field | Description | Unit |
| --- | --- | --- |
| Object | the type of road agents (Car/Pedestrian) |  |
| Location | The location of the center of the road agent (x, y) | meters |
| Rotation | The rotation of the road agent | degree |
| Dimension | The dimensions of the road agent (Length, Width, Height) | meters |

  ## ğŸ¯ Goal

Process the raw camera and LiDAR data from both vehicles to:


ğŸ”¹ Generate individual object detection outputs for each car.

ğŸ”¹ Fuse the data to build a shared perception of the scene.

ğŸ”¹ Enhance visibility by addressing sensor occlusions and inconsistencies.

ğŸ”¹ Output a visual representation showing detected agents from both perspectives.

## ğŸ” GUI & Frontend what tech is used

(Dummy text regarding the GUI)

ğŸ”¹ LiDAR provides accurate 3D data but lacks color information.

ğŸ”¹ Cameras capture rich visuals but are sensitive to lighting conditions.

## ğŸ” GUI & Frontend

The dashboard interface is built using **PyQt6** for a modern, responsive car dashboard look.

### âš™ï¸ Tech Stack

- **Language:** Python 3
- **GUI Framework:** [PyQt6](https://pypi.org/project/PyQt6/)
- **Image Handling:** Qt (QPixmap), OpenCV
- **Assets:** All UI graphics (like the top-view car) are loaded from the `assets/` folder

### ğŸ–¥ï¸ Features

- **Live speed, arrival time, and power display** â€” values update dynamically, simulating a real car dashboard.
- **Obstacle detection alert** â€” shows real-time warnings like `"DANGER: Obstacle within 10m"` in a bold red card if any object is too close.
- **Image slideshow** â€” cycles through camera images from the dataset (from `data/CameraA/`), showing the carâ€™s current view.
- **Theme toggle** â€” switch between dark and light modes with a single button.
- **Modern styling** â€” custom fonts, color-coded status, sidebar icons for navigation, and responsive layout.

### ğŸ—ï¸ Folder Structure

- `frontend.py` â€” main application file (run this to start the dashboard)
- `assets/` â€” UI images (e.g. `car_top_view.png`)
- `data/CameraA/` â€” slideshow camera images shown in the interface

## ğŸ“¸ Output Example: Sensor Fusion Visualization

This example shows the output of our perception pipeline, fusing LiDAR and camera data to detect obstacles and visualize the scene:

![Sensor Fusion Output](assets/lidar-fusion.png)
